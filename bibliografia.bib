@dataset{phamhieuhuyVinDrMammoLargescaleBenchmark,
  title = {{{VinDr-Mammo}}: {{A}} Large-Scale Benchmark Dataset for Computer-Aided Detection and Diagnosis in Full-Field Digital Mammography},
  shorttitle = {{{VinDr-Mammo}}},
  author = {Pham, Hieu Huy and Nguyen Trung, Hieu and Nguyen, Ha Quy},
  publisher = {{PhysioNet}},
  doi = {10.13026/BR2V-7517},
  url = {https://physionet.org/content/vindr-mammo/1.0.0/},
  urldate = {2022-11-08},
  abstract = {Breast cancer is one of the most prevalent types of cancer and the leading type of cancer death. Mammography is the recommended imaging modality for periodic breast cancer screening. A few datasets have been published to develop computer-aided tools for mammography analysis. However, these datasets either have a limited sample size or consist of screen-film mammography (SFM), which have been replaced by full-field digital mammography (FFDM) in clinical practices. This project introduces a large-scale full-field digital mammography dataset of 5,000 four-view exams, which are double read by experienced mammographers to provide cancer assessment and breast density following the Breast Imaging Report and Data System (BI-RADS). Breast abnormalities that require further examination are also marked by bounding rectangles.},
  version = {1.0.0}
}

@inproceedings{pizerContrastlimitedAdaptiveHistogram1990,
  title = {Contrast-Limited Adaptive Histogram Equalization: Speed and Effectiveness},
  shorttitle = {Contrast-Limited Adaptive Histogram Equalization},
  booktitle = {[1990] {{Proceedings}} of the {{First Conference}} on {{Visualization}} in {{Biomedical Computing}}},
  author = {Pizer, S.M. and Johnston, R.E. and Ericksen, J.P. and Yankaskas, B.C. and Muller, K.E.},
  date = {1990-05},
  pages = {337--345},
  doi = {10.1109/VBC.1990.109340},
  url = {https://ieeexplore.ieee.org/document/109340},
  urldate = {2023-10-03},
  abstract = {An experiment intended to evaluate the clinical application of contrast-limited adaptive histogram equalization (CLAHE) to chest computer tomography (CT) images is reported. A machine especially designed to compute CLAHE in a few seconds is discussed. It is shown that CLAHE can be computed in 4 s after 5-s loading time using the specially designed parallel engine made from a few thousand dollars worth of off-the-shelf components. The processing appears to be useful for a wide range of medical images, but the limitations of observer calibration make it impossible to demonstrate such usefulness by agreement experiments.{$<>$}},
  eventtitle = {[1990] {{Proceedings}} of the {{First Conference}} on {{Visualization}} in {{Biomedical Computing}}},
  file = {/home/dmelladoc/Zotero/storage/KI55MHM7/109340.html}
}

@report{ministeriodesaludInformeVigilanciaCancer2021,
  title = {Informe de {{Vigilancia}} de {{Cáncer}}. {{Análisis}} de {{Mortalidad Prematura}} y {{AVPP}} Por {{Cáncer}}. {{Década}} 2009-2018.},
  author = {{Ministerio de Salud}},
  date = {2021},
  institution = {{Departamento de Epidemiologia}},
  location = {{Chile}},
  url = {https://www.minsal.cl/wp-content/uploads/2022/01/Informe-Mortalidad-Prematura-y-AVPP-por-C%C3%A1ncer-2009-2018.pdf},
  urldate = {2023-11-09},
  file = {/home/dmelladoc/Zotero/storage/45U8R6N7/Informe-Mortalidad-Prematura-y-AVPP-por-Cáncer-2009-2018.pdf}
}

@online{BreastCancerStatistics,
  title = {Breast Cancer Statistics | {{World Cancer Research Fund International}}},
  url = {https://www.wcrf.org/cancer-trends/breast-cancer-statistics/},
  urldate = {2023-11-09},
  abstract = {Latest statistics on breast cancer, with data on which countries have the highest rates, plus advice on preventing breast cancer.},
  langid = {american},
  organization = {{WCRF International}},
  file = {/home/dmelladoc/Zotero/storage/F8ALAPLY/breast-cancer-statistics.html}
}

@article{castilloResultadosTratamientoCancer2017,
  title = {Resultados Del Tratamiento Del Cáncer de Mama, {{Programa Nacional}} de {{Cáncer}} Del {{Adulto}}},
  author = {Castillo, César del SM and Cabrera, M. Elena C. and Derio P., Lea and Gaete V., Fancy and Cavada CH., Gabriel and Castillo, César del SM and Cabrera, M. Elena C. and Derio P., Lea and Gaete V., Fancy and Cavada CH., Gabriel},
  date = {2017-12},
  journaltitle = {Revista médica de Chile},
  volume = {145},
  number = {12},
  pages = {1507--1513},
  publisher = {{Sociedad Médica de Santiago}},
  issn = {0034-9887},
  doi = {10.4067/s0034-98872017001201507},
  url = {https://scielo.conicyt.cl/scielo.php?script=sci_abstract&pid=S0034-98872017001201507&lng=es&nrm=iso&tlng=en},
  urldate = {2021-04-21},
  file = {/home/dmelladoc/Zotero/storage/TEJZVUH8/Castillo et al. - 2017 - Resultados del tratamiento del cáncer de mama, Pro.pdf;/home/dmelladoc/Zotero/storage/3NYXSF5X/scielo.html}
}


@article{rodriguez-ruizDetectionBreastCancer2018,
  title = {Detection of {{Breast Cancer}} with {{Mammography}}: {{Effect}} of an {{Artificial Intelligence Support System}}},
  shorttitle = {Detection of {{Breast Cancer}} with {{Mammography}}},
  author = {Rodríguez-Ruiz, Alejandro and Krupinski, Elizabeth and Mordang, Jan-Jurre and Schilling, Kathy and Heywang-Köbrunner, Sylvia H. and Sechopoulos, Ioannis and Mann, Ritse M.},
  date = {2018-11-20},
  journaltitle = {Radiology},
  shortjournal = {Radiology},
  volume = {290},
  number = {2},
  pages = {305--314},
  publisher = {{Radiological Society of North America}},
  issn = {0033-8419},
  doi = {10.1148/radiol.2018181371},
  url = {https://pubs.rsna.org/doi/10.1148/radiol.2018181371},
  urldate = {2021-05-12},
  abstract = {PurposeTo compare breast cancer detection performance of radiologists reading mammographic examinations unaided versus supported by an artificial intelligence (AI) system.Materials and MethodsAn enriched retrospective, fully crossed, multireader, multicase, HIPAA-compliant study was performed. Screening digital mammographic examinations from 240 women (median age, 62 years; range, 39–89 years) performed between 2013 and 2017 were included. The 240 examinations (100 showing cancers, 40 leading to false-positive recalls, 100 normal) were interpreted by 14 Mammography Quality Standards Act–qualified radiologists, once with and once without AI support. The readers provided a Breast Imaging Reporting and Data System score and probability of malignancy. AI support provided radiologists with interactive decision support (clicking on a breast region yields a local cancer likelihood score), traditional lesion markers for computer-detected abnormalities, and an examination-based cancer likelihood score. The area under the receiver operating characteristic curve (AUC), specificity and sensitivity, and reading time were compared between conditions by using mixed-models analysis dof variance and generalized linear models for multiple repeated measurements.ResultsOn average, the AUC was higher with AI support than with unaided reading (0.89 vs 0.87, respectively; P = .002). Sensitivity increased with AI support (86\% [86 of 100] vs 83\% [83 of 100]; P = .046), whereas specificity trended toward improvement (79\% [111 of 140]) vs 77\% [108 of 140]; P = .06). Reading time per case was similar (unaided, 146 seconds; supported by AI, 149 seconds; P = .15). The AUC with the AI system alone was similar to the average AUC of the radiologists (0.89 vs 0.87).ConclusionRadiologists improved their cancer detection at mammography when using an artificial intelligence system for support, without requiring additional reading time.Published under a CC BY 4.0 license.See also the editorial by Bahl in this issue.},
  file = {/home/dmelladoc/Zotero/storage/88A324MF/Rodríguez-Ruiz et al. - 2018 - Detection of Breast Cancer with Mammography Effec.pdf;/home/dmelladoc/Zotero/storage/8VRIJCZQ/radiol.html}
}

@article{diazSonSistemasInteligencia2021,
  title = {¿Son los sistemas de inteligencia artificial una herramienta útil para los programas de cribado de cáncer de mama?},
  author = {Díaz, O. and Rodríguez-Ruiz, A. and Gubern-Mérida, A. and Martí, R. and Chevalier, M.},
  date = {2021-05},
  journaltitle = {Radiología},
  shortjournal = {Radiología},
  volume = {63},
  number = {3},
  pages = {236--244},
  issn = {00338338},
  doi = {10.1016/j.rx.2020.11.006},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0033833820301752},
  urldate = {2023-03-30},
  langid = {spanish},
  file = {/home/dmelladoc/Zotero/storage/ZTF3VXW4/Díaz et al. - 2021 - ¿Son los sistemas de inteligencia artificial una h.pdf}
}

@inproceedings{ertosunProbabilisticVisualSearch2015,
  title = {Probabilistic Visual Search for Masses within Mammography Images Using Deep Learning},
  booktitle = {2015 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  author = {Ertosun, Mehmet Gunhan and Rubin, Daniel L.},
  date = {2015-11},
  pages = {1310--1315},
  publisher = {{IEEE}},
  location = {{Washington, DC, USA}},
  doi = {10.1109/BIBM.2015.7359868},
  url = {http://ieeexplore.ieee.org/document/7359868/},
  urldate = {2023-11-09},
  abstract = {We developed a deep learning-based visual search system for the task of automated search and localization of masses in whole mammography images. The system consists of two modules: a classification engine and a localization engine. It first classifies mammograms as containing a mass or no mass using a deep learning classifier, and then localizes the mass(es) within the image using a regional probabilistic approach based on a deep learning network. We obtained 85\% accuracy for the task of identifying images that contain a mass, and we were able to localize 85\% of the masses at an average of 0.9 false positives per image. Our system has the advantages of being able to work with an entire mammography image as input without the need for image segmentation or other pre-processing steps, such as cropping or tiling the image, and it is based on deep learning with unsupervised feature discovery, so it does not require pre-defined and hand-crafted image features.},
  eventtitle = {2015 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  isbn = {978-1-4673-6799-8},
  langid = {english},
  file = {/home/dmelladoc/Zotero/storage/C8TKLP7R/Ertosun y Rubin - 2015 - Probabilistic visual search for masses within mamm.pdf}
}

@article{frankDeepLearningArchitecture2023,
  title = {A Deep Learning Architecture with an Object-Detection Algorithm and a Convolutional Neural Network for Breast Mass Detection and Visualization},
  author = {Frank, Steven J.},
  date = {2023-11-01},
  journaltitle = {Healthcare Analytics},
  shortjournal = {Healthcare Analytics},
  volume = {3},
  pages = {100186},
  issn = {2772-4425},
  doi = {10.1016/j.health.2023.100186},
  url = {https://www.sciencedirect.com/science/article/pii/S2772442523000539},
  urldate = {2023-11-09},
  abstract = {This study presents an integrated deep learning architecture with an object-detection algorithm and a convolutional neural network (CNN) for breast mass detection and visualization. Mammograms are analyzed to identify and localize breast mass lesions to aid clinician review. Two complementary forms of deep learning are used to identify the regions of interest (ROIs). An object-detection algorithm, YOLO v5, analyzes the entire mammogram to identify discrete image regions likely to represent masses. Object detections exhibit high precision, but the object-detection stage alone has insufficient overall accuracy for a clinical application. A CNN independently analyzes the mammogram after it has been decomposed into subregion tiles and is trained to emphasize sensitivity (recall). The ROIs identified by each analysis are highlighted in different colors to facilitate an efficient staged review. The CNN stage nearly always detects tumor masses when present but typically occupies a larger area of the image. By inspecting the high-precision regions followed by the high-sensitivity regions, clinicians can quickly identify likely lesions before completing the review of the full mammogram. On average, the ROIs occupy less than 20\% of the tissue in the mammograms, even without removing pectoral muscle from the analysis. As a result, the proposed system helps clinicians review mammograms with greater accuracy and efficiency.},
  keywords = {Convolutional neural network,Decision support,Deep learning,Intelligence,Object detection,Radiology},
  file = {/home/dmelladoc/Zotero/storage/NI9ZJPDG/Frank - 2023 - A deep learning architecture with an object-detect.pdf;/home/dmelladoc/Zotero/storage/NWJ68T7G/S2772442523000539.html}
}

@online{yiOptimizingVisualizingDeep2017,
  title = {Optimizing and {{Visualizing Deep Learning}} for {{Benign}}/{{Malignant Classification}} in {{Breast Tumors}}},
  author = {Yi, Darvin and Sawyer, Rebecca Lynn and Cohn III, David and Dunnmon, Jared and Lam, Carson and Xiao, Xuerong and Rubin, Daniel},
  date = {2017-05-17},
  eprint = {1705.06362},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1705.06362},
  urldate = {2023-11-09},
  abstract = {Breast cancer has the highest incidence and second highest mortality rate for women in the US. Our study aims to utilize deep learning for benign/malignant classification of mammogram tumors using a subset of cases from the Digital Database of Screening Mammography (DDSM). Though it was a small dataset from the view of Deep Learning (∼ 1000 patients), we show that currently state of the art architectures of deep learning can find a robust signal, even when trained from scratch. Using convolutional neural networks (CNNs), we are able to achieve an accuracy of 85\% and an ROC AUC of 0.91, while leading hand-crafted feature based methods are only able to achieve an accuracy of 71\%. We investigate an amalgamation of architectures to show that our best result is reached with an ensemble of the lightweight GoogLe Nets tasked with interpreting both the coronal caudal view and the mediolateral oblique view, simply averaging the probability scores of both views to make the final prediction. In addition, we have created a novel method to visualize what features the neural network detects for the benign/malignant classification, and have correlated those features with well known radiological features, such as spiculation. Our algorithm significantly improves existing classification methods for mammography lesions and identifies features that correlate with established clinical markers.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/dmelladoc/Zotero/storage/YJ3SB36F/Yi et al. - 2017 - Optimizing and Visualizing Deep Learning for Benig.pdf}
}


@online{tanEfficientNetV2SmallerModels2021,
  title = {{{EfficientNetV2}}: {{Smaller Models}} and {{Faster Training}}},
  shorttitle = {{{EfficientNetV2}}},
  author = {Tan, Mingxing and Le, Quoc V.},
  date = {2021-06-23},
  eprint = {2104.00298},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2104.00298},
  urldate = {2023-11-09},
  abstract = {This paper introduces EfficientNetV2, a new family of convolutional networks that have faster training speed and better parameter efficiency than previous models. To develop these models, we use a combination of training-aware neural architecture search and scaling, to jointly optimize training speed and parameter efficiency. The models were searched from the search space enriched with new ops such as Fused-MBConv. Our experiments show that EfficientNetV2 models train much faster than state-of-the-art models while being up to 6.8x smaller.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/dmelladoc/Zotero/storage/RPVC7ZZM/Tan y Le - 2021 - EfficientNetV2 Smaller Models and Faster Training.pdf}
}

@online{heDeepResidualLearning2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2015-12-10},
  eprint = {1512.03385},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1512.03385},
  url = {http://arxiv.org/abs/1512.03385},
  urldate = {2023-11-10},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/dmelladoc/Zotero/storage/D9WHETWK/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf;/home/dmelladoc/Zotero/storage/X4N3KNLX/1512.html}
}

@online{huangDenselyConnectedConvolutional2018,
  title = {Densely {{Connected Convolutional Networks}}},
  author = {Huang, Gao and Liu, Zhuang and family=Maaten, given=Laurens, prefix=van der, useprefix=true and Weinberger, Kilian Q.},
  date = {2018-01-28},
  eprint = {1608.06993},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1608.06993},
  url = {http://arxiv.org/abs/1608.06993},
  urldate = {2023-11-10},
  abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/dmelladoc/Zotero/storage/ZUGS846Y/Huang et al. - 2018 - Densely Connected Convolutional Networks.pdf}
}

@online{liuSwinTransformerHierarchical2021,
  title = {Swin {{Transformer}}: {{Hierarchical Vision Transformer}} Using {{Shifted Windows}}},
  shorttitle = {Swin {{Transformer}}},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  date = {2021-08-17},
  eprint = {2103.14030},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2103.14030},
  url = {http://arxiv.org/abs/2103.14030},
  urldate = {2023-11-10},
  abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \textbackslash textbf\{S\}hifted \textbackslash textbf\{win\}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at\textasciitilde\textbackslash url\{https://github.com/microsoft/Swin-Transformer\}.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/dmelladoc/Zotero/storage/EPNSIYPW/Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer .pdf}
}

@online{howardMobileNetsEfficientConvolutional2017,
  title = {{{MobileNets}}: {{Efficient Convolutional Neural Networks}} for {{Mobile Vision Applications}}},
  shorttitle = {{{MobileNets}}},
  author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  date = {2017-04-16},
  eprint = {1704.04861},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1704.04861},
  url = {http://arxiv.org/abs/1704.04861},
  urldate = {2023-11-10},
  abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/dmelladoc/Zotero/storage/BQUGSXG7/Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Network.pdf}
}

@online{simonyanVeryDeepConvolutional2015,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  date = {2015-04-10},
  eprint = {1409.1556},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1409.1556},
  url = {http://arxiv.org/abs/1409.1556},
  urldate = {2023-11-10},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  pubstate = {preprint},
  version = {6},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/dmelladoc/Zotero/storage/MECU8ZYU/Simonyan y Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf}
}

