@dataset{phamhieuhuyVinDrMammoLargescaleBenchmark,
  title = {{{VinDr-Mammo}}: {{A}} Large-Scale Benchmark Dataset for Computer-Aided Detection and Diagnosis in Full-Field Digital Mammography},
  shorttitle = {{{VinDr-Mammo}}},
  author = {Pham, Hieu Huy and Nguyen Trung, Hieu and Nguyen, Ha Quy},
  publisher = {{PhysioNet}},
  doi = {10.13026/BR2V-7517},
  url = {https://physionet.org/content/vindr-mammo/1.0.0/},
  urldate = {2022-11-08},
  abstract = {Breast cancer is one of the most prevalent types of cancer and the leading type of cancer death. Mammography is the recommended imaging modality for periodic breast cancer screening. A few datasets have been published to develop computer-aided tools for mammography analysis. However, these datasets either have a limited sample size or consist of screen-film mammography (SFM), which have been replaced by full-field digital mammography (FFDM) in clinical practices. This project introduces a large-scale full-field digital mammography dataset of 5,000 four-view exams, which are double read by experienced mammographers to provide cancer assessment and breast density following the Breast Imaging Report and Data System (BI-RADS). Breast abnormalities that require further examination are also marked by bounding rectangles.},
  version = {1.0.0}
}

@inproceedings{pizerContrastlimitedAdaptiveHistogram1990,
  title = {Contrast-Limited Adaptive Histogram Equalization: Speed and Effectiveness},
  shorttitle = {Contrast-Limited Adaptive Histogram Equalization},
  booktitle = {[1990] {{Proceedings}} of the {{First Conference}} on {{Visualization}} in {{Biomedical Computing}}},
  author = {Pizer, S.M. and Johnston, R.E. and Ericksen, J.P. and Yankaskas, B.C. and Muller, K.E.},
  date = {1990-05},
  pages = {337--345},
  doi = {10.1109/VBC.1990.109340},
  url = {https://ieeexplore.ieee.org/document/109340},
  urldate = {2023-10-03},
  abstract = {An experiment intended to evaluate the clinical application of contrast-limited adaptive histogram equalization (CLAHE) to chest computer tomography (CT) images is reported. A machine especially designed to compute CLAHE in a few seconds is discussed. It is shown that CLAHE can be computed in 4 s after 5-s loading time using the specially designed parallel engine made from a few thousand dollars worth of off-the-shelf components. The processing appears to be useful for a wide range of medical images, but the limitations of observer calibration make it impossible to demonstrate such usefulness by agreement experiments.{$<>$}},
  eventtitle = {[1990] {{Proceedings}} of the {{First Conference}} on {{Visualization}} in {{Biomedical Computing}}},
  file = {/home/dmelladoc/Zotero/storage/KI55MHM7/109340.html}
}

@report{ministeriodesaludInformeVigilanciaCancer2021,
  title = {Informe de {{Vigilancia}} de {{Cáncer}}. {{Análisis}} de {{Mortalidad Prematura}} y {{AVPP}} Por {{Cáncer}}. {{Década}} 2009-2018.},
  author = {{Ministerio de Salud}},
  date = {2021},
  institution = {{Departamento de Epidemiologia}},
  location = {{Chile}},
  url = {https://www.minsal.cl/wp-content/uploads/2022/01/Informe-Mortalidad-Prematura-y-AVPP-por-C%C3%A1ncer-2009-2018.pdf},
  urldate = {2023-11-09},
  file = {/home/dmelladoc/Zotero/storage/45U8R6N7/Informe-Mortalidad-Prematura-y-AVPP-por-Cáncer-2009-2018.pdf}
}

@online{BreastCancerStatistics,
  title = {Breast Cancer Statistics | {{World Cancer Research Fund International}}},
  url = {https://www.wcrf.org/cancer-trends/breast-cancer-statistics/},
  urldate = {2023-11-09},
  abstract = {Latest statistics on breast cancer, with data on which countries have the highest rates, plus advice on preventing breast cancer.},
  langid = {american},
  organization = {{WCRF International}},
  file = {/home/dmelladoc/Zotero/storage/F8ALAPLY/breast-cancer-statistics.html}
}

@article{castilloResultadosTratamientoCancer2017,
  title = {Resultados Del Tratamiento Del Cáncer de Mama, {{Programa Nacional}} de {{Cáncer}} Del {{Adulto}}},
  author = {Castillo, César del SM and Cabrera, M. Elena C. and Derio P., Lea and Gaete V., Fancy and Cavada CH., Gabriel and Castillo, César del SM and Cabrera, M. Elena C. and Derio P., Lea and Gaete V., Fancy and Cavada CH., Gabriel},
  date = {2017-12},
  journaltitle = {Revista médica de Chile},
  volume = {145},
  number = {12},
  pages = {1507--1513},
  publisher = {{Sociedad Médica de Santiago}},
  issn = {0034-9887},
  doi = {10.4067/s0034-98872017001201507},
  url = {https://scielo.conicyt.cl/scielo.php?script=sci_abstract&pid=S0034-98872017001201507&lng=es&nrm=iso&tlng=en},
  urldate = {2021-04-21},
  file = {/home/dmelladoc/Zotero/storage/TEJZVUH8/Castillo et al. - 2017 - Resultados del tratamiento del cáncer de mama, Pro.pdf;/home/dmelladoc/Zotero/storage/3NYXSF5X/scielo.html}
}


@article{rodriguez-ruizDetectionBreastCancer2018,
  title = {Detection of {{Breast Cancer}} with {{Mammography}}: {{Effect}} of an {{Artificial Intelligence Support System}}},
  shorttitle = {Detection of {{Breast Cancer}} with {{Mammography}}},
  author = {Rodríguez-Ruiz, Alejandro and Krupinski, Elizabeth and Mordang, Jan-Jurre and Schilling, Kathy and Heywang-Köbrunner, Sylvia H. and Sechopoulos, Ioannis and Mann, Ritse M.},
  date = {2018-11-20},
  journaltitle = {Radiology},
  shortjournal = {Radiology},
  volume = {290},
  number = {2},
  pages = {305--314},
  publisher = {{Radiological Society of North America}},
  issn = {0033-8419},
  doi = {10.1148/radiol.2018181371},
  url = {https://pubs.rsna.org/doi/10.1148/radiol.2018181371},
  urldate = {2021-05-12},
  abstract = {PurposeTo compare breast cancer detection performance of radiologists reading mammographic examinations unaided versus supported by an artificial intelligence (AI) system.Materials and MethodsAn enriched retrospective, fully crossed, multireader, multicase, HIPAA-compliant study was performed. Screening digital mammographic examinations from 240 women (median age, 62 years; range, 39–89 years) performed between 2013 and 2017 were included. The 240 examinations (100 showing cancers, 40 leading to false-positive recalls, 100 normal) were interpreted by 14 Mammography Quality Standards Act–qualified radiologists, once with and once without AI support. The readers provided a Breast Imaging Reporting and Data System score and probability of malignancy. AI support provided radiologists with interactive decision support (clicking on a breast region yields a local cancer likelihood score), traditional lesion markers for computer-detected abnormalities, and an examination-based cancer likelihood score. The area under the receiver operating characteristic curve (AUC), specificity and sensitivity, and reading time were compared between conditions by using mixed-models analysis dof variance and generalized linear models for multiple repeated measurements.ResultsOn average, the AUC was higher with AI support than with unaided reading (0.89 vs 0.87, respectively; P = .002). Sensitivity increased with AI support (86\% [86 of 100] vs 83\% [83 of 100]; P = .046), whereas specificity trended toward improvement (79\% [111 of 140]) vs 77\% [108 of 140]; P = .06). Reading time per case was similar (unaided, 146 seconds; supported by AI, 149 seconds; P = .15). The AUC with the AI system alone was similar to the average AUC of the radiologists (0.89 vs 0.87).ConclusionRadiologists improved their cancer detection at mammography when using an artificial intelligence system for support, without requiring additional reading time.Published under a CC BY 4.0 license.See also the editorial by Bahl in this issue.},
  file = {/home/dmelladoc/Zotero/storage/88A324MF/Rodríguez-Ruiz et al. - 2018 - Detection of Breast Cancer with Mammography Effec.pdf;/home/dmelladoc/Zotero/storage/8VRIJCZQ/radiol.html}
}

@article{diazSonSistemasInteligencia2021,
  title = {¿Son los sistemas de inteligencia artificial una herramienta útil para los programas de cribado de cáncer de mama?},
  author = {Díaz, O. and Rodríguez-Ruiz, A. and Gubern-Mérida, A. and Martí, R. and Chevalier, M.},
  date = {2021-05},
  journaltitle = {Radiología},
  shortjournal = {Radiología},
  volume = {63},
  number = {3},
  pages = {236--244},
  issn = {00338338},
  doi = {10.1016/j.rx.2020.11.006},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0033833820301752},
  urldate = {2023-03-30},
  langid = {spanish},
  file = {/home/dmelladoc/Zotero/storage/ZTF3VXW4/Díaz et al. - 2021 - ¿Son los sistemas de inteligencia artificial una h.pdf}
}

@inproceedings{ertosunProbabilisticVisualSearch2015,
  title = {Probabilistic Visual Search for Masses within Mammography Images Using Deep Learning},
  booktitle = {2015 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  author = {Ertosun, Mehmet Gunhan and Rubin, Daniel L.},
  date = {2015-11},
  pages = {1310--1315},
  publisher = {{IEEE}},
  location = {{Washington, DC, USA}},
  doi = {10.1109/BIBM.2015.7359868},
  url = {http://ieeexplore.ieee.org/document/7359868/},
  urldate = {2023-11-09},
  abstract = {We developed a deep learning-based visual search system for the task of automated search and localization of masses in whole mammography images. The system consists of two modules: a classification engine and a localization engine. It first classifies mammograms as containing a mass or no mass using a deep learning classifier, and then localizes the mass(es) within the image using a regional probabilistic approach based on a deep learning network. We obtained 85\% accuracy for the task of identifying images that contain a mass, and we were able to localize 85\% of the masses at an average of 0.9 false positives per image. Our system has the advantages of being able to work with an entire mammography image as input without the need for image segmentation or other pre-processing steps, such as cropping or tiling the image, and it is based on deep learning with unsupervised feature discovery, so it does not require pre-defined and hand-crafted image features.},
  eventtitle = {2015 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  isbn = {978-1-4673-6799-8},
  langid = {english},
  file = {/home/dmelladoc/Zotero/storage/C8TKLP7R/Ertosun y Rubin - 2015 - Probabilistic visual search for masses within mamm.pdf}
}

@article{frankDeepLearningArchitecture2023,
  title = {A Deep Learning Architecture with an Object-Detection Algorithm and a Convolutional Neural Network for Breast Mass Detection and Visualization},
  author = {Frank, Steven J.},
  date = {2023-11-01},
  journaltitle = {Healthcare Analytics},
  shortjournal = {Healthcare Analytics},
  volume = {3},
  pages = {100186},
  issn = {2772-4425},
  doi = {10.1016/j.health.2023.100186},
  url = {https://www.sciencedirect.com/science/article/pii/S2772442523000539},
  urldate = {2023-11-09},
  abstract = {This study presents an integrated deep learning architecture with an object-detection algorithm and a convolutional neural network (CNN) for breast mass detection and visualization. Mammograms are analyzed to identify and localize breast mass lesions to aid clinician review. Two complementary forms of deep learning are used to identify the regions of interest (ROIs). An object-detection algorithm, YOLO v5, analyzes the entire mammogram to identify discrete image regions likely to represent masses. Object detections exhibit high precision, but the object-detection stage alone has insufficient overall accuracy for a clinical application. A CNN independently analyzes the mammogram after it has been decomposed into subregion tiles and is trained to emphasize sensitivity (recall). The ROIs identified by each analysis are highlighted in different colors to facilitate an efficient staged review. The CNN stage nearly always detects tumor masses when present but typically occupies a larger area of the image. By inspecting the high-precision regions followed by the high-sensitivity regions, clinicians can quickly identify likely lesions before completing the review of the full mammogram. On average, the ROIs occupy less than 20\% of the tissue in the mammograms, even without removing pectoral muscle from the analysis. As a result, the proposed system helps clinicians review mammograms with greater accuracy and efficiency.},
  keywords = {Convolutional neural network,Decision support,Deep learning,Intelligence,Object detection,Radiology},
  file = {/home/dmelladoc/Zotero/storage/NI9ZJPDG/Frank - 2023 - A deep learning architecture with an object-detect.pdf;/home/dmelladoc/Zotero/storage/NWJ68T7G/S2772442523000539.html}
}

@online{yiOptimizingVisualizingDeep2017,
  title = {Optimizing and {{Visualizing Deep Learning}} for {{Benign}}/{{Malignant Classification}} in {{Breast Tumors}}},
  author = {Yi, Darvin and Sawyer, Rebecca Lynn and Cohn III, David and Dunnmon, Jared and Lam, Carson and Xiao, Xuerong and Rubin, Daniel},
  date = {2017-05-17},
  eprint = {1705.06362},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1705.06362},
  urldate = {2023-11-09},
  abstract = {Breast cancer has the highest incidence and second highest mortality rate for women in the US. Our study aims to utilize deep learning for benign/malignant classification of mammogram tumors using a subset of cases from the Digital Database of Screening Mammography (DDSM). Though it was a small dataset from the view of Deep Learning (∼ 1000 patients), we show that currently state of the art architectures of deep learning can find a robust signal, even when trained from scratch. Using convolutional neural networks (CNNs), we are able to achieve an accuracy of 85\% and an ROC AUC of 0.91, while leading hand-crafted feature based methods are only able to achieve an accuracy of 71\%. We investigate an amalgamation of architectures to show that our best result is reached with an ensemble of the lightweight GoogLe Nets tasked with interpreting both the coronal caudal view and the mediolateral oblique view, simply averaging the probability scores of both views to make the final prediction. In addition, we have created a novel method to visualize what features the neural network detects for the benign/malignant classification, and have correlated those features with well known radiological features, such as spiculation. Our algorithm significantly improves existing classification methods for mammography lesions and identifies features that correlate with established clinical markers.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/dmelladoc/Zotero/storage/YJ3SB36F/Yi et al. - 2017 - Optimizing and Visualizing Deep Learning for Benig.pdf}
}


@online{tanEfficientNetV2SmallerModels2021,
  title = {{{EfficientNetV2}}: {{Smaller Models}} and {{Faster Training}}},
  shorttitle = {{{EfficientNetV2}}},
  author = {Tan, Mingxing and Le, Quoc V.},
  date = {2021-06-23},
  eprint = {2104.00298},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2104.00298},
  urldate = {2023-11-09},
  abstract = {This paper introduces EfficientNetV2, a new family of convolutional networks that have faster training speed and better parameter efficiency than previous models. To develop these models, we use a combination of training-aware neural architecture search and scaling, to jointly optimize training speed and parameter efficiency. The models were searched from the search space enriched with new ops such as Fused-MBConv. Our experiments show that EfficientNetV2 models train much faster than state-of-the-art models while being up to 6.8x smaller.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/dmelladoc/Zotero/storage/RPVC7ZZM/Tan y Le - 2021 - EfficientNetV2 Smaller Models and Faster Training.pdf}
}
